# quora_question_similarity

How to use

File is divided in three parts
1) Model training
2) Data prediction
3) Plus one model.png file which shows the structure of the model.

How to use Model_training
1) You need to first creat model and tokenizer to run data prediction file.
2) To do that first download stanford glove vector file and copy glove.6B.300d.txt it in Model_training.
3) After this when you will run my_model.py model and tokenizer will be created. You need to copy these models and tokenizer in Data prediction.
4) Do not forgrt to rename these to my_model_9.h5, mymodel_tok.

How to use Data prediction

1) from predictor_func import similarity_similarity_score.
2) It takes 4 arguments:- question1, question2, custom stopwords and a boolean if you want to add extra stopwords.
3) It returns two numbers first is one if the two questions are exactly the same and zero if not second number is the similarity score.
4) You can create and send custom stopwords or you can use from custum_stopwords import custum_stopwords_creat func to create a list of possible stopwords.
5) custum_stopwords.custum_stopwords_creat takes two arguments first is the file converted in pandas data frame from which you want to make stopwords and second is the number of stopwords you want to create.
6) It is very important that input data which contains list of questions have an index named ‘Question_text’. Otherwise it will not recognise the questions.
7) To see how it works you can run predict.py by simply typing python predict.py.
8) Here input file are already present in the folder.

Dependencies
1) requirements.txt can be usd to solve all dependency problems.
2) You also need to download wordnet and stopwords. Which can be done by command:-
  python -m nltk.downloader
  This will open nltk window no you have to type < d wordnet stopwords >

Why I chose this model
1) Test accuracy given by this model after 9 epochs is 84.6%. Which is very good.
2) All the other models I tried gave less accuracy. Like MaLSTM gave 82% accuracy.
3) This is a relatively lightweight model considering the other models.


What made this model successful?

1) Features generated by LSTM is also question order independent. They share the same LSTM layer. After the LSTM layer, output of question1 and question2 merged with commutative operations which are square of difference and summation.
2) I think a good preprocessing on the questions also leads to better generalization.
3) The neural network is not so big and has reasonable amount of dropouts and gaussiannoises.


Reference materials
1) This model is based on the model from:-https://github.com/aerdem4/kaggle-quora-dup.
2) Andrew Ng course can also be used at coursera :-https://www.coursera.org/learn/nlp-sequence-models/home/welcome. You have to audit this course in order to learn it.
3) To download quora question pairs click on:- http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv
4) To download word vector click on:- http://nlp.stanford.edu/data/glove.6B.zip 

If you face any problem regarding this file feel free to ask.
